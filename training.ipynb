{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unified Reasoning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from task import Task\n",
    "from unified_module import UnifiedReasoningModule\n",
    "import random\n",
    "import torch.multiprocessing as mp\n",
    "import traceback\n",
    "from trainer import train_model, load_precomputed_tasks\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Set device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths\n",
    "MODEL_DIR = \"output/models/unified_shape\"\n",
    "DATA_DIR = \"precomputed_tasks/training\"\n",
    "METRICS_DIR = \"output/metrics\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(METRICS_DIR, exist_ok=True)\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Load tasks\n",
    "    print(\"Loading tasks...\")\n",
    "    tasks = load_precomputed_tasks(DATA_DIR)\n",
    "    print(f\"Loaded {len(tasks)} tasks\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize model\n",
    "        print(\"Initializing unified model...\")\n",
    "        model = UnifiedReasoningModule(\n",
    "            input_dim=3,\n",
    "            hidden_dim=40,\n",
    "            output_dim=11,\n",
    "            device=device\n",
    "        )\n",
    "        model.model = model.model.to(device)\n",
    "        \n",
    "        # Train model with task-aware approach\n",
    "        print(\"Starting unified model training...\")\n",
    "        trained_model, history = train_model(\n",
    "            model=model,\n",
    "            tasks=tasks,\n",
    "            num_epochs=10,\n",
    "            learning_rate=0.02,\n",
    "            weight_decay=1e-5,\n",
    "            save_dir=MODEL_DIR,\n",
    "            model_name=\"unified_shape\",\n",
    "            batch_size=8,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        print(\"Training complete!\")\n",
    "        return trained_model, history\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main function: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "# Run the main function when executed\n",
    "if __name__ == \"__main__\":\n",
    "    mp.set_start_method('spawn', force=True)\n",
    "    model, history = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from task import Task\n",
    "from unified_module import UnifiedReasoningModule\n",
    "import random\n",
    "import traceback\n",
    "from meta_learning import run_meta_learning, MAMLTrainer, ProtoNetTrainer\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Set device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths\n",
    "pretrained_model_path = \"output/models/unified_reasoning_model_final.pt\"\n",
    "MODEL_DIR = \"output/models/unified\"\n",
    "TRAIN_DIR = \"data/training\"\n",
    "VAL_DIR = \"data/evaluation\"\n",
    "METRICS_DIR = \"output/metrics/unified_meta\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(METRICS_DIR, exist_ok=True)\n",
    "\n",
    "# Load tasks\n",
    "def load_tasks(directory):\n",
    "    \"\"\"Load tasks from directory\"\"\"\n",
    "    tasks = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, \"r\") as f:\n",
    "                    data = json.load(f)\n",
    "                    if \"train\" not in data or \"test\" not in data:\n",
    "                        print(f\"Warning: Invalid task format in {file_path}\")\n",
    "                        continue\n",
    "                    \n",
    "                    task = Task(\n",
    "                        task_id=os.path.basename(file_path),\n",
    "                        train_pairs=[(pair[\"input\"], pair[\"output\"]) for pair in data[\"train\"]],\n",
    "                        test_pairs=[(pair[\"input\"], pair[\"output\"]) for pair in data[\"test\"]],\n",
    "                    )\n",
    "                    tasks.append(task)\n",
    "    return tasks\n",
    "\n",
    "# Split tasks into training and validation sets\n",
    "def split_tasks(tasks, val_ratio=0.2):\n",
    "    \"\"\"Split tasks into training and validation sets\"\"\"\n",
    "    random.shuffle(tasks)\n",
    "    val_size = int(len(tasks) * val_ratio)\n",
    "    train_tasks = tasks[val_size:]\n",
    "    val_tasks = tasks[:val_size]\n",
    "    return train_tasks, val_tasks\n",
    "\n",
    "# Train meta-learning models\n",
    "def train_meta_learning(model, train_tasks, val_tasks, method=\"both\"):\n",
    "    \"\"\"\n",
    "    Train meta-learning models.\n",
    "    \n",
    "    Args:\n",
    "        model: UnifiedReasoningModule instance\n",
    "        train_tasks: List of training tasks\n",
    "        val_tasks: List of validation tasks\n",
    "        method: Which method to use (\"maml\", \"proto\", or \"both\")\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of trained models\n",
    "    \"\"\"\n",
    "    trained_models = {}\n",
    "    \n",
    "    if method.lower() == \"maml\" or method.lower() == \"both\":\n",
    "        print(\"Training MAML model...\")\n",
    "        maml_trainer = run_meta_learning(\n",
    "            unified_model=model,\n",
    "            train_tasks=train_tasks,\n",
    "            val_tasks=val_tasks,\n",
    "            method=\"maml\",\n",
    "            epochs=50,\n",
    "            lr=0.001,\n",
    "            weight_decay=1e-5,\n",
    "            log_dir=os.path.join(MODEL_DIR, \"maml\")\n",
    "        )\n",
    "        trained_models[\"maml\"] = maml_trainer\n",
    "    \n",
    "    if method.lower() == \"proto\" or method.lower() == \"both\":\n",
    "        print(\"Training Prototypical Networks model...\")\n",
    "        proto_trainer = run_meta_learning(\n",
    "            unified_model=model,\n",
    "            train_tasks=train_tasks,\n",
    "            val_tasks=val_tasks,\n",
    "            method=\"proto\",\n",
    "            epochs=50,\n",
    "            lr=0.001,\n",
    "            weight_decay=1e-5,\n",
    "            log_dir=os.path.join(MODEL_DIR, \"proto\")\n",
    "        )\n",
    "        trained_models[\"proto\"] = proto_trainer\n",
    "    \n",
    "    return trained_models\n",
    "\n",
    "# Test on new tasks\n",
    "def test_meta_learning(trainer, test_tasks):\n",
    "    \"\"\"\n",
    "    Test meta-learning models on new tasks.\n",
    "    \n",
    "    Args:\n",
    "        trainer: Meta-learning trainer instance\n",
    "        test_tasks: List of test tasks\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of test metrics\n",
    "    \"\"\"\n",
    "    print(f\"Testing {trainer.__class__.__name__} on {len(test_tasks)} tasks...\")\n",
    "    \n",
    "    task_metrics = []\n",
    "    avg_accuracy = 0.0\n",
    "    \n",
    "    for i, task in enumerate(test_tasks):\n",
    "        try:\n",
    "            # Prepare support and query sets\n",
    "            support_graphs = task.train_graphs\n",
    "            query_graphs = task.test_graphs\n",
    "            \n",
    "            if not support_graphs or not query_graphs:\n",
    "                print(f\"Skipping task {i} due to empty graphs\")\n",
    "                continue\n",
    "                \n",
    "            # Create batches\n",
    "            support_batch = torch.geometric.data.Batch.from_data_list(support_graphs).to(device)\n",
    "            query_batch = torch.geometric.data.Batch.from_data_list(query_graphs).to(device)\n",
    "            \n",
    "            # Evaluate with few-shot adaptation\n",
    "            accuracy = trainer.validate_task(support_batch, query_batch)\n",
    "            \n",
    "            print(f\"Task {i} accuracy: {accuracy:.4f}\")\n",
    "            task_metrics.append({\n",
    "                \"task_id\": task.task_id,\n",
    "                \"accuracy\": accuracy,\n",
    "                \"num_train\": len(support_graphs),\n",
    "                \"num_test\": len(query_graphs)\n",
    "            })\n",
    "            \n",
    "            avg_accuracy += accuracy\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error testing task {i}: {e}\")\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    avg_accuracy /= len(task_metrics) if task_metrics else 1.0\n",
    "    print(f\"Average test accuracy: {avg_accuracy:.4f}\")\n",
    "    \n",
    "    # Save metrics\n",
    "    method_name = trainer.__class__.__name__.replace(\"Trainer\", \"\").lower()\n",
    "    metrics_path = os.path.join(METRICS_DIR, f\"{method_name}_test_metrics.json\")\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump({\n",
    "            \"avg_accuracy\": avg_accuracy,\n",
    "            \"task_metrics\": task_metrics\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    return {\n",
    "        \"avg_accuracy\": avg_accuracy,\n",
    "        \"task_metrics\": task_metrics\n",
    "    }\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Load tasks\n",
    "    print(\"Loading tasks...\")\n",
    "    train_tasks = load_tasks(TRAIN_DIR)\n",
    "    val_tasks = load_tasks(VAL_DIR)\n",
    "    print(f\"Loaded {len(train_tasks)+len(val_tasks)} tasks\")\n",
    "    \n",
    "    # Split into train, validation, and test sets\n",
    "    val_tasks, test_tasks = split_tasks(val_tasks, val_ratio=0.5)\n",
    "    print(f\"Split into {len(train_tasks)} training, {len(val_tasks)} validation, and {len(test_tasks)} test tasks\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize model\n",
    "        print(\"Initializing unified model...\")\n",
    "        model = UnifiedReasoningModule(\n",
    "            input_dim=3,\n",
    "            hidden_dim=128,\n",
    "            output_dim=11,\n",
    "            device=device\n",
    "        )\n",
    "        model.load_complete_state(pretrained_model_path)\n",
    "        model.model = model.model.to(device)\n",
    "        \n",
    "        # Train meta-learning models\n",
    "        print(\"Training meta-learning models...\")\n",
    "        trained_models = train_meta_learning(\n",
    "            model=model,\n",
    "            train_tasks=train_tasks,\n",
    "            val_tasks=val_tasks,\n",
    "            method=\"both\"  # Train both MAML and Prototypical Networks\n",
    "        )\n",
    "        \n",
    "        # Test meta-learning models\n",
    "        print(\"Testing meta-learning models...\")\n",
    "        test_results = {}\n",
    "        for method, trainer in trained_models.items():\n",
    "            test_results[method] = test_meta_learning(trainer, test_tasks)\n",
    "        \n",
    "        # Compare results\n",
    "        print(\"\\nComparison of meta-learning methods:\")\n",
    "        for method, results in test_results.items():\n",
    "            print(f\"{method.upper()}: Average accuracy = {results['avg_accuracy']:.4f}\")\n",
    "        \n",
    "        print(\"Meta-learning training and evaluation complete!\")\n",
    "        return trained_models, test_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main function: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "# Run the main function when executed\n",
    "if __name__ == \"__main__\":\n",
    "    models, results = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### NLM Reasoning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from task5 import Task\n",
    "from nlm_shape import NLMReasoningModule\n",
    "import random\n",
    "import torch.multiprocessing as mp\n",
    "import traceback\n",
    "from functools import partial\n",
    "from trainer import train_model, load_precomputed_tasks\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Set device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths\n",
    "MODEL_DIR = \"output/models/nlm_shape\"\n",
    "DATA_DIR = \"precomputed_tasks/training\"\n",
    "METRICS_DIR = \"output/metrics\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(METRICS_DIR, exist_ok=True)\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Load tasks\n",
    "    print(\"Loading tasks...\")\n",
    "    tasks = load_precomputed_tasks(DATA_DIR)\n",
    "    print(f\"Loaded {len(tasks)} tasks\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize model\n",
    "        print(\"Initializing nlm model...\")\n",
    "        model = NLMReasoningModule(\n",
    "            input_dim=3,\n",
    "            hidden_dim=128,\n",
    "            output_dim=11,\n",
    "            device=device\n",
    "        )\n",
    "        model.model = model.model.to(device)\n",
    "        \n",
    "        # Train model with task-aware approach\n",
    "        print(\"Starting nlm model training...\")\n",
    "        trained_model, history = train_model(\n",
    "            model=model,\n",
    "            tasks=tasks,\n",
    "            num_epochs=1000,\n",
    "            learning_rate=0.0001,\n",
    "            weight_decay=1e-5,\n",
    "            save_dir=MODEL_DIR,\n",
    "            model_name=\"nlm_shape\",\n",
    "            batch_size=8,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        print(\"Training complete!\")\n",
    "        return trained_model, history\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main function: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "# Run the main function when executed\n",
    "if __name__ == \"__main__\":\n",
    "    mp.set_start_method('spawn', force=True)\n",
    "    model, history = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from task4 import Task, Blackboard\n",
    "from unified_module import UnifiedReasoningModule\n",
    "from trainer import load_precomputed_tasks\n",
    "import random\n",
    "import traceback\n",
    "from meta_learning import run_meta_learning, MAMLTrainer, ProtoNetTrainer\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Set device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths\n",
    "MODEL_DIR = \"output/models/nlm\"\n",
    "TRAIN_DIR = \"precomputed_tasks/training_400\"\n",
    "EVAL_DIR = \"precomputed_tasks/evaluation_400\"\n",
    "METRICS_DIR = \"output/metrics/nlm\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(METRICS_DIR, exist_ok=True)\n",
    "\n",
    "# Split tasks into training and validation sets\n",
    "def split_tasks(tasks, val_ratio=0.2):\n",
    "    \"\"\"Split tasks into training and validation sets\"\"\"\n",
    "    random.shuffle(tasks)\n",
    "    val_size = int(len(tasks) * val_ratio)\n",
    "    train_tasks = tasks[val_size:]\n",
    "    val_tasks = tasks[:val_size]\n",
    "    return train_tasks, val_tasks\n",
    "\n",
    "# Train meta-learning models\n",
    "def train_meta_learning(model, train_tasks, val_tasks, method=\"both\"):\n",
    "    \"\"\"\n",
    "    Train meta-learning models.\n",
    "    \n",
    "    Args:\n",
    "        model: Reasoning module instance\n",
    "        train_tasks: List of training tasks\n",
    "        val_tasks: List of validation tasks\n",
    "        method: Which method to use (\"maml\", \"proto\", or \"both\")\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of trained models\n",
    "    \"\"\"\n",
    "    trained_models = {}\n",
    "    \n",
    "    if method.lower() == \"maml\" or method.lower() == \"both\":\n",
    "        print(\"Training MAML model...\")\n",
    "        maml_trainer = run_meta_learning(\n",
    "            reasoning_module=model,\n",
    "            train_tasks=train_tasks,\n",
    "            val_tasks=val_tasks,\n",
    "            method=\"maml\",\n",
    "            epochs=50,\n",
    "            lr=0.001,\n",
    "            weight_decay=1e-5,\n",
    "            log_dir=os.path.join(MODEL_DIR, \"maml\")\n",
    "        )\n",
    "        trained_models[\"maml\"] = maml_trainer\n",
    "    \n",
    "    if method.lower() == \"proto\" or method.lower() == \"both\":\n",
    "        print(\"Training Prototypical Networks model...\")\n",
    "        proto_trainer = run_meta_learning(\n",
    "            reasoning_module=model,\n",
    "            train_tasks=train_tasks,\n",
    "            val_tasks=val_tasks,\n",
    "            method=\"proto\",\n",
    "            epochs=50,\n",
    "            lr=0.001,\n",
    "            weight_decay=1e-5,\n",
    "            log_dir=os.path.join(MODEL_DIR, \"proto\")\n",
    "        )\n",
    "        trained_models[\"proto\"] = proto_trainer\n",
    "    \n",
    "    return trained_models\n",
    "\n",
    "# Test on new tasks\n",
    "def test_meta_learning(trainer, test_tasks):\n",
    "    \"\"\"\n",
    "    Test meta-learning models on new tasks.\n",
    "    \n",
    "    Args:\n",
    "        trainer: Meta-learning trainer instance\n",
    "        test_tasks: List of test tasks\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of test metrics\n",
    "    \"\"\"\n",
    "    print(f\"Testing {trainer.__class__.__name__} on {len(test_tasks)} tasks...\")\n",
    "    \n",
    "    task_metrics = []\n",
    "    avg_accuracy = 0.0\n",
    "    \n",
    "    for i, task in enumerate(test_tasks):\n",
    "        try:\n",
    "            # Prepare support and query sets\n",
    "            support_graphs = task.train_graphs\n",
    "            query_graphs = task.test_graphs\n",
    "            \n",
    "            if not support_graphs or not query_graphs:\n",
    "                print(f\"Skipping task {i} due to empty graphs\")\n",
    "                continue\n",
    "                \n",
    "            # Create batches\n",
    "            support_batch = Batch.from_data_list(support_graphs).to(device)\n",
    "            query_batch = Batch.from_data_list(query_graphs).to(device)\n",
    "            \n",
    "            # Evaluate with few-shot adaptation\n",
    "            accuracy = trainer.validate_task(support_batch, query_batch)\n",
    "            \n",
    "            print(f\"Task {i} accuracy: {accuracy:.4f}\")\n",
    "            task_metrics.append({\n",
    "                \"task_id\": task.task_id,\n",
    "                \"accuracy\": accuracy,\n",
    "                \"num_train\": len(support_graphs),\n",
    "                \"num_test\": len(query_graphs)\n",
    "            })\n",
    "            \n",
    "            avg_accuracy += accuracy\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error testing task {i}: {e}\")\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    avg_accuracy /= len(task_metrics) if task_metrics else 1.0\n",
    "    print(f\"Average test accuracy: {avg_accuracy:.4f}\")\n",
    "    \n",
    "    # Save metrics\n",
    "    method_name = trainer.__class__.__name__.replace(\"Trainer\", \"\").lower()\n",
    "    metrics_path = os.path.join(METRICS_DIR, f\"{method_name}_test_metrics.json\")\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump({\n",
    "            \"avg_accuracy\": avg_accuracy,\n",
    "            \"task_metrics\": task_metrics\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    return {\n",
    "        \"avg_accuracy\": avg_accuracy,\n",
    "        \"task_metrics\": task_metrics\n",
    "    }\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Load tasks\n",
    "    print(\"Loading tasks...\")\n",
    "    tasks = load_precomputed_tasks(TRAIN_DIR)\n",
    "    test_tasks = load_precomputed_tasks(EVAL_DIR)\n",
    "    \n",
    "    # Split into train, validation, and test sets\n",
    "    train_tasks, val_tasks = split_tasks(tasks, val_ratio=0.2)\n",
    "    print(f\"Split into {len(train_tasks)} training, {len(val_tasks)} validation, and {len(test_tasks)} test tasks\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize model\n",
    "        print(\"Initializing unified model...\")\n",
    "        model = UnifiedReasoningModule(\n",
    "            input_dim=3,\n",
    "            hidden_dim=128,\n",
    "            output_dim=11,\n",
    "            device=device\n",
    "        )\n",
    "        model.load_complete_state(os.path.join(MODEL_DIR, \"unified_reasoning_model_final.pt\"))\n",
    "        model.model = model.model.to(device)\n",
    "        \n",
    "        # Train meta-learning models\n",
    "        print(\"Training meta-learning models...\")\n",
    "        trained_models = train_meta_learning(\n",
    "            model=model,\n",
    "            train_tasks=train_tasks,\n",
    "            val_tasks=val_tasks,\n",
    "            method=\"maml\"\n",
    "        )\n",
    "        \n",
    "        # Test meta-learning models\n",
    "        print(\"Testing meta-learning models...\")\n",
    "        test_results = {}\n",
    "        for method, trainer in trained_models.items():\n",
    "            test_results[method] = test_meta_learning(trainer, test_tasks)\n",
    "        \n",
    "        # Compare results\n",
    "        print(\"\\nComparison of meta-learning methods:\")\n",
    "        for method, results in test_results.items():\n",
    "            print(f\"{method.upper()}: Average accuracy = {results['avg_accuracy']:.4f}\")\n",
    "        \n",
    "        print(\"Meta-learning training and evaluation complete!\")\n",
    "        return trained_models, test_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main function: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "# Run the main function when executed\n",
    "if __name__ == \"__main__\":\n",
    "    models, results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from task4 import Task\n",
    "from unified_module import UnifiedReasoningModule\n",
    "from trainer import load_precomputed_tasks\n",
    "import random\n",
    "import traceback\n",
    "from meta_learning import run_meta_learning, MAMLTrainer, ProtoNetTrainer\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Set device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths\n",
    "MODEL_DIR = \"output/models/unified\"\n",
    "TRAIN_DIR = \"precomputed_tasks/training_400\"\n",
    "EVAL_DIR = \"precomputed_tasks/evaluation_400\"\n",
    "METRICS_DIR = \"output/metrics/unified\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(METRICS_DIR, exist_ok=True)\n",
    "\n",
    "# Function to create k-fold splits\n",
    "def create_kfold_splits(tasks, k=5, random_state=42):\n",
    "    \"\"\"Create k-fold splits of tasks for cross-validation\"\"\"\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Create indices for the splits\n",
    "    task_indices = np.arange(len(tasks))\n",
    "    splits = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(task_indices):\n",
    "        # Get actual tasks for this fold\n",
    "        fold_train_tasks = [tasks[i] for i in train_idx]\n",
    "        fold_val_tasks = [tasks[i] for i in val_idx]\n",
    "        splits.append((fold_train_tasks, fold_val_tasks))\n",
    "    \n",
    "    return splits\n",
    "\n",
    "# Train meta-learning models with k-fold cross-validation\n",
    "def train_meta_learning_kfold(model_creator, tasks, test_tasks, method=\"maml\", k=5, epochs=30):\n",
    "    \"\"\"\n",
    "    Train meta-learning models using k-fold cross-validation.\n",
    "    \n",
    "    Args:\n",
    "        model_creator: Function to create a fresh model instance\n",
    "        tasks: List of all training tasks\n",
    "        test_tasks: List of test tasks for final evaluation\n",
    "        method: Which method to use (\"maml\" or \"proto\")\n",
    "        k: Number of folds for cross-validation\n",
    "        epochs: Number of training epochs per fold\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with performance metrics\n",
    "    \"\"\"\n",
    "    # Create k-fold splits\n",
    "    splits = create_kfold_splits(tasks, k=k)\n",
    "    \n",
    "    # Store metrics for each fold\n",
    "    fold_metrics = []\n",
    "    best_model = None\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    # Train and evaluate on each fold\n",
    "    for fold_idx, (fold_train_tasks, fold_val_tasks) in enumerate(splits):\n",
    "        print(f\"\\n=== Training Fold {fold_idx+1}/{k} ===\")\n",
    "        print(f\"Train tasks: {len(fold_train_tasks)}, Validation tasks: {len(fold_val_tasks)}\")\n",
    "        \n",
    "        # Create a fresh model instance for this fold\n",
    "        fold_model = model_creator()\n",
    "        \n",
    "        # Train the model on this fold\n",
    "        try:\n",
    "            fold_trainer = run_meta_learning(\n",
    "                reasoning_module=fold_model,\n",
    "                train_tasks=fold_train_tasks,\n",
    "                val_tasks=fold_val_tasks,\n",
    "                method=method,\n",
    "                epochs=epochs,\n",
    "                lr=0.001,\n",
    "                weight_decay=1e-5,\n",
    "                log_dir=os.path.join(MODEL_DIR, f\"{method}_fold{fold_idx+1}\")\n",
    "            )\n",
    "            \n",
    "            # Evaluate on validation set\n",
    "            val_accuracy = evaluate_tasks(fold_trainer, fold_val_tasks)\n",
    "            \n",
    "            # Save metrics for this fold\n",
    "            fold_metrics.append({\n",
    "                \"fold\": fold_idx+1,\n",
    "                \"val_accuracy\": val_accuracy,\n",
    "                \"train_tasks\": len(fold_train_tasks),\n",
    "                \"val_tasks\": len(fold_val_tasks),\n",
    "                \"train_losses\": fold_trainer.train_losses[-10:],  # Last 10 epochs\n",
    "                \"val_accuracies\": fold_trainer.val_accuracies[-10:]  # Last 10 epochs\n",
    "            })\n",
    "            \n",
    "            print(f\"Fold {fold_idx+1} validation accuracy: {val_accuracy:.4f}\")\n",
    "            \n",
    "            # Keep track of best model\n",
    "            if val_accuracy > best_val_acc:\n",
    "                best_val_acc = val_accuracy\n",
    "                best_model = fold_trainer\n",
    "                print(f\"New best model with validation accuracy: {val_accuracy:.4f}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error training fold {fold_idx+1}: {e}\")\n",
    "            traceback.print_exc()\n",
    "            \n",
    "    # Save best model\n",
    "    if best_model is not None:\n",
    "        best_model_path = os.path.join(MODEL_DIR, f\"{method}_best_model.pt\")\n",
    "        torch.save({\n",
    "            'model_state_dict': best_model.reasoning_module.model.state_dict(),\n",
    "            'model_type': type(best_model.reasoning_module).__name__,\n",
    "            'meta_learning_method': method,\n",
    "            'val_accuracy': float(best_val_acc),\n",
    "            'test_accuracy': float(test_accuracy),\n",
    "            'train_losses': best_model.train_losses,\n",
    "            'train_accuracies': best_model.train_accuracies,\n",
    "            'val_accuracies': best_model.val_accuracies,\n",
    "        }, best_model_path)\n",
    "        print(f\"Best {method} model saved to {best_model_path}\")\n",
    "    \n",
    "    # Compute average metrics across folds\n",
    "    avg_val_accuracy = np.mean([m[\"val_accuracy\"] for m in fold_metrics])\n",
    "    print(f\"\\nAverage validation accuracy across {k} folds: {avg_val_accuracy:.4f}\")\n",
    "    \n",
    "    # Evaluate best model on test set\n",
    "    if best_model is not None:\n",
    "        test_accuracy = evaluate_tasks(best_model, test_tasks)\n",
    "        print(f\"Best model test accuracy: {test_accuracy:.4f}\")\n",
    "    else:\n",
    "        test_accuracy = 0.0\n",
    "        print(\"No valid model found for testing\")\n",
    "    \n",
    "    # Save comprehensive metrics\n",
    "    metrics = {\n",
    "        \"method\": method,\n",
    "        \"avg_val_accuracy\": float(avg_val_accuracy),\n",
    "        \"test_accuracy\": float(test_accuracy),\n",
    "        \"fold_metrics\": fold_metrics,\n",
    "        \"best_fold_val_accuracy\": float(best_val_acc),\n",
    "        \"k\": k\n",
    "    }\n",
    "    \n",
    "    metrics_path = os.path.join(METRICS_DIR, f\"{method}_kfold_metrics.json\")\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    \n",
    "    return metrics, best_model\n",
    "\n",
    "# Evaluate model on a set of tasks\n",
    "def evaluate_tasks(trainer, tasks):\n",
    "    \"\"\"\n",
    "    Evaluate a meta-learning model on a set of tasks.\n",
    "    \n",
    "    Args:\n",
    "        trainer: Meta-learning trainer instance\n",
    "        tasks: List of tasks to evaluate on\n",
    "        \n",
    "    Returns:\n",
    "        Average accuracy across tasks\n",
    "    \"\"\"\n",
    "    accuracies = []\n",
    "    \n",
    "    for task in tqdm(tasks, desc=\"Evaluating tasks\"):\n",
    "        try:\n",
    "            # Prepare support and query sets\n",
    "            support_graphs = task.train_graphs\n",
    "            query_graphs = task.test_graphs\n",
    "            \n",
    "            if not support_graphs or not query_graphs:\n",
    "                continue\n",
    "                \n",
    "            # Create batches\n",
    "            support_batch = Batch.from_data_list(support_graphs).to(device)\n",
    "            query_batch = Batch.from_data_list(query_graphs).to(device)\n",
    "            \n",
    "            # Evaluate with few-shot adaptation\n",
    "            accuracy = trainer.validate_task(support_batch, query_batch, task)\n",
    "            accuracies.append(accuracy)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating task: {e}\")\n",
    "    \n",
    "    # Calculate average accuracy\n",
    "    avg_accuracy = np.mean(accuracies) if accuracies else 0.0\n",
    "    return avg_accuracy\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Load tasks\n",
    "    print(\"Loading tasks...\")\n",
    "    all_train_tasks = load_precomputed_tasks(TRAIN_DIR)\n",
    "    test_tasks = load_precomputed_tasks(EVAL_DIR)\n",
    "    print(f\"Loaded {len(all_train_tasks)} training tasks and {len(test_tasks)} evaluation tasks\")\n",
    "    \n",
    "    # Shuffle tasks for better randomization\n",
    "    random.shuffle(all_train_tasks)\n",
    "    \n",
    "    try:\n",
    "        # Function to create a fresh model instance\n",
    "        def create_model():\n",
    "            model = UnifiedReasoningModule(\n",
    "                input_dim=3,\n",
    "                hidden_dim=128,\n",
    "                output_dim=11,\n",
    "                device=device\n",
    "            )\n",
    "            # Load pre-trained weights\n",
    "            model.load_complete_state(os.path.join(MODEL_DIR, \"unified_reasoning_model_final.pt\"))\n",
    "            model.model = model.model.to(device)\n",
    "            return model\n",
    "        \n",
    "        # Initialize the first model\n",
    "        print(\"Initializing unified model...\")\n",
    "        base_model = create_model()\n",
    "        \n",
    "        # Train MAML with 5-fold cross-validation\n",
    "        print(\"\\n=== Training MAML with 5-fold cross-validation ===\")\n",
    "        maml_metrics, best_maml = train_meta_learning_kfold(\n",
    "            model_creator=create_model,\n",
    "            tasks=all_train_tasks,\n",
    "            test_tasks=test_tasks,\n",
    "            method=\"maml\",\n",
    "            k=5,\n",
    "            epochs=50\n",
    "        )\n",
    "        \n",
    "        # # Optionally train Prototypical Networks with 5-fold cross-validation\n",
    "        # print(\"\\n=== Training Prototypical Networks with 5-fold cross-validation ===\")\n",
    "        # proto_metrics, best_proto = train_meta_learning_kfold(\n",
    "        #     model_creator=create_model,\n",
    "        #     tasks=all_train_tasks,\n",
    "        #     test_tasks=test_tasks,\n",
    "        #     method=\"proto\",\n",
    "        #     k=5,\n",
    "        #     epochs=30\n",
    "        # )\n",
    "        \n",
    "        # Compare results\n",
    "        print(\"\\n=== Final Results ===\")\n",
    "        print(f\"MAML: Average validation accuracy = {maml_metrics['avg_val_accuracy']:.4f}, Test accuracy = {maml_metrics['test_accuracy']:.4f}\")\n",
    "        # print(f\"ProtoNet: Average validation accuracy = {proto_metrics['avg_val_accuracy']:.4f}, Test accuracy = {proto_metrics['test_accuracy']:.4f}\")\n",
    "        \n",
    "        # Return best models and metrics\n",
    "        return {\n",
    "            \"maml\": (best_maml, maml_metrics),\n",
    "            \"proto\": (best_proto, proto_metrics)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main function: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Run the main function when executed\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from task4 import Task, Blackboard\n",
    "from unified_module import UnifiedReasoningModule\n",
    "from nlm_module import NLMReasoningModule\n",
    "from trainer import load_precomputed_tasks\n",
    "import random\n",
    "import traceback\n",
    "from meta_learning import run_meta_learning, MAMLTrainer, ProtoNetTrainer\n",
    "from torch_geometric.data import Batch\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Set device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths\n",
    "MODEL_DIR = \"output/models\"\n",
    "TRAIN_DIR = \"precomputed_tasks/training_400\"\n",
    "EVAL_DIR = \"precomputed_tasks/evaluation_400\"\n",
    "METRICS_DIR = \"output/metrics\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(METRICS_DIR, exist_ok=True)\n",
    "\n",
    "# Create k-fold split of tasks\n",
    "def create_kfold_splits(tasks, k=5):\n",
    "    \"\"\"Create k-fold splits of tasks\"\"\"\n",
    "    random.shuffle(tasks)\n",
    "    fold_size = len(tasks) // k\n",
    "    folds = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        start_idx = i * fold_size\n",
    "        end_idx = (i + 1) * fold_size if i < k - 1 else len(tasks)\n",
    "        val_tasks = tasks[start_idx:end_idx]\n",
    "        train_tasks = tasks[:start_idx] + tasks[end_idx:]\n",
    "        folds.append((train_tasks, val_tasks))\n",
    "    \n",
    "    return folds\n",
    "\n",
    "# Initialize a new model for each fold\n",
    "def initialize_model(model_type, model_path, input_dim=3, hidden_dim=128, output_dim=11):\n",
    "    \"\"\"Create a new model instance with fresh weights\"\"\"\n",
    "    if model_type == \"unified\":\n",
    "        model = UnifiedReasoningModule(\n",
    "            input_dim=input_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            output_dim=output_dim,\n",
    "            device=device\n",
    "        )\n",
    "        model.load_complete_state(model_path)\n",
    "        print(\"Model weights loaded successfully\")\n",
    "        \n",
    "    elif model_type == \"nlm\":\n",
    "        model = NLMReasoningModule(\n",
    "            input_dim=input_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            output_dim=output_dim,\n",
    "            device=device\n",
    "        )\n",
    "        model.load_complete_state(model_path)\n",
    "        print(\"Model weights loaded successfully\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "        \n",
    "    model.model = model.model.to(device)\n",
    "    return model\n",
    "\n",
    "# Train meta-learning models with k-fold cross-validation\n",
    "def train_kfold_meta_learning(model_type, model_path, train_dir, test_dir, k=5, method=\"maml\"):\n",
    "    \"\"\"\n",
    "    Train meta-learning models using k-fold cross-validation.\n",
    "    \n",
    "    Args:\n",
    "        model_type: Type of model to use (\"unified\" or \"nlm\")\n",
    "        train_dir: Directory containing training tasks\n",
    "        test_dir: Directory containing test tasks\n",
    "        k: Number of folds\n",
    "        method: Which meta-learning method to use (\"maml\" or \"proto\")\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of trained models and fold metrics\n",
    "    \"\"\"\n",
    "    # Create model-specific directories\n",
    "    model_dir = os.path.join(MODEL_DIR, model_type)\n",
    "    metrics_dir = os.path.join(METRICS_DIR, model_type)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    os.makedirs(metrics_dir, exist_ok=True)\n",
    "    \n",
    "    # Log directory for this specific model-method combination\n",
    "    method_log_dir = os.path.join(model_dir, method)\n",
    "    os.makedirs(method_log_dir, exist_ok=True)\n",
    "    \n",
    "    # Load all tasks initially just to create the folds\n",
    "    all_tasks = load_precomputed_tasks(train_dir)\n",
    "    \n",
    "    # Create k-fold splits (just the task indices, not the actual task objects)\n",
    "    all_indices = list(range(len(all_tasks)))\n",
    "    random.shuffle(all_indices)\n",
    "    \n",
    "    fold_size = len(all_indices) // k\n",
    "    fold_indices = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        start_idx = i * fold_size\n",
    "        end_idx = (i + 1) * fold_size if i < k - 1 else len(all_indices)\n",
    "        val_indices = all_indices[start_idx:end_idx]\n",
    "        train_indices = [idx for idx in all_indices if idx not in val_indices]\n",
    "        fold_indices.append((train_indices, val_indices))\n",
    "    \n",
    "    fold_results = []\n",
    "    best_model = None\n",
    "    best_val_accuracy = 0.0\n",
    "    \n",
    "    # Train on each fold\n",
    "    for fold_idx, (train_indices, val_indices) in enumerate(fold_indices):\n",
    "        print(f\"\\n--- Training {model_type.upper()} with {method.upper()} on Fold {fold_idx+1}/{k} ---\")\n",
    "        \n",
    "        # Reload all tasks fresh for this fold\n",
    "        all_tasks = load_precomputed_tasks(train_dir)\n",
    "        \n",
    "        # Split into train and validation based on indices\n",
    "        train_tasks = [all_tasks[i] for i in train_indices]\n",
    "        val_tasks = [all_tasks[i] for i in val_indices]\n",
    "        \n",
    "        print(f\"Train tasks: {len(train_tasks)}, Validation tasks: {len(val_tasks)}\")\n",
    "        \n",
    "        # Initialize a fresh model for this fold\n",
    "        model = initialize_model(model_type, model_path)\n",
    "        \n",
    "        # Create a specific log directory for this fold\n",
    "        fold_log_dir = os.path.join(method_log_dir, f\"fold{fold_idx+1}\")\n",
    "        os.makedirs(fold_log_dir, exist_ok=True)\n",
    "\n",
    "        # Before running MAML training, prepare the data properly\n",
    "        if model_type == \"nlm\":\n",
    "            # Process train and validation tasks for NLM\n",
    "            for task in train_tasks:\n",
    "                # This ensures the processed graphs have consistent shapes\n",
    "                if hasattr(model, '_prepare_training_data'):\n",
    "                    # First make sure the task data is in the right format for NLM\n",
    "                    for graph in task.train_graphs + task.test_graphs:\n",
    "                        # Ensure x has the right shape and type for NLM\n",
    "                        if hasattr(graph, 'x'):\n",
    "                            if graph.x.dim() == 2 and graph.x.size(1) == 11:\n",
    "                                graph.x = graph.x.argmax(dim=1)\n",
    "                            \n",
    "                            # Convert to long type and reshape if needed\n",
    "                            graph.x = graph.x.long()\n",
    "                            if graph.x.dim() == 1:\n",
    "                                graph.x = graph.x.unsqueeze(1)\n",
    "                            \n",
    "                            # Add position info if needed\n",
    "                            if not hasattr(graph, 'pos') and graph.x.size(1) >= 3:\n",
    "                                graph.pos = graph.x[:, 1:3].float()\n",
    "                    \n",
    "                    # Similar preprocessing for validation tasks\n",
    "                    for val_task in val_tasks:\n",
    "                        for graph in val_task.train_graphs + val_task.test_graphs:\n",
    "                            if hasattr(graph, 'x'):\n",
    "                                if graph.x.dim() == 2 and graph.x.size(1) == 11:\n",
    "                                    graph.x = graph.x.argmax(dim=1)\n",
    "                                graph.x = graph.x.long()\n",
    "                                if graph.x.dim() == 1:\n",
    "                                    graph.x = graph.x.unsqueeze(1)\n",
    "                                if not hasattr(graph, 'pos') and graph.x.size(1) >= 3:\n",
    "                                    graph.pos = graph.x[:, 1:3].float()\n",
    "        \n",
    "        # Train on this fold\n",
    "        trainer = run_meta_learning(\n",
    "            reasoning_module=model,\n",
    "            train_tasks=train_tasks,\n",
    "            val_tasks=val_tasks,\n",
    "            method=method,\n",
    "            epochs=50,\n",
    "            lr=0.001,\n",
    "            weight_decay=1e-5,\n",
    "            log_dir=fold_log_dir\n",
    "        )\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        val_accuracy = evaluate_model(trainer, val_tasks, method=method)\n",
    "        print(f\"Fold {fold_idx+1} validation accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "        # Save fold results\n",
    "        fold_results.append({\n",
    "            \"fold\": fold_idx + 1,\n",
    "            \"model_type\": model_type,\n",
    "            \"method\": method,\n",
    "            \"val_accuracy\": val_accuracy,\n",
    "            \"num_train\": len(train_tasks),\n",
    "            \"num_val\": len(val_tasks)\n",
    "        })\n",
    "        \n",
    "        # Keep track of best model\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_model = trainer\n",
    "            # Save the best model for this fold\n",
    "            best_model_path = os.path.join(method_log_dir, f\"fold{fold_idx+1}_best.pt\")\n",
    "            torch.save({\n",
    "                \"model_type\": model_type,\n",
    "                \"method\": method,\n",
    "                \"fold\": fold_idx + 1,\n",
    "                \"accuracy\": val_accuracy,\n",
    "                \"model_state_dict\": model.model.state_dict(),\n",
    "                \"trainer_type\": type(trainer).__name__\n",
    "            }, best_model_path)\n",
    "            print(f\"New best model with validation accuracy: {val_accuracy:.4f}, saved to {best_model_path}\")\n",
    "        \n",
    "        # Clean up memory\n",
    "        del train_tasks\n",
    "        del val_tasks\n",
    "        del all_tasks\n",
    "        torch.cuda.empty_cache()  # Clear CUDA cache if using GPU\n",
    "    \n",
    "    # Calculate average validation accuracy across folds\n",
    "    avg_val_accuracy = np.mean([fold[\"val_accuracy\"] for fold in fold_results])\n",
    "    print(f\"\\n{model_type.upper()} with {method.upper()}: Average validation accuracy across {k} folds: {avg_val_accuracy:.4f}\")\n",
    "    \n",
    "    # Load fresh test tasks and evaluate the best model\n",
    "    test_tasks = load_precomputed_tasks(test_dir)\n",
    "    print(f\"Evaluating best {model_type.upper()} model with {method.upper()} on {len(test_tasks)} test tasks...\")\n",
    "    \n",
    "    if best_model is not None:\n",
    "        test_accuracy = evaluate_model(best_model, test_tasks, method=method)\n",
    "        print(f\"Best {model_type.upper()} model with {method.upper()} test accuracy: {test_accuracy:.4f}\")\n",
    "    else:\n",
    "        test_accuracy = 0.0\n",
    "        print(f\"No best {model_type.upper()} model found\")\n",
    "    \n",
    "    # Save k-fold metrics\n",
    "    metrics_path = os.path.join(metrics_dir, f\"{method}_metrics.json\")\n",
    "    metrics_data = {\n",
    "        \"model_type\": model_type,\n",
    "        \"method\": method,\n",
    "        \"avg_val_accuracy\": avg_val_accuracy,\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"fold_results\": fold_results\n",
    "    }\n",
    "    \n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump(metrics_data, f, indent=2)\n",
    "    print(f\"Metrics saved to {metrics_path}\")\n",
    "    \n",
    "    return {\n",
    "        \"model_type\": model_type,\n",
    "        \"method\": method,\n",
    "        \"best_model\": best_model,\n",
    "        \"avg_val_accuracy\": avg_val_accuracy,\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"fold_results\": fold_results\n",
    "    }\n",
    "\n",
    "# Evaluate model on a set of tasks\n",
    "def evaluate_model(trainer, tasks, method=\"maml\"):\n",
    "    \"\"\"\n",
    "    Evaluate a meta-learning model on a set of tasks.\n",
    "    \n",
    "    Args:\n",
    "        trainer: Meta-learning trainer instance\n",
    "        tasks: List of tasks to evaluate on\n",
    "        method: Meta-learning method name\n",
    "        \n",
    "    Returns:\n",
    "        Average accuracy across tasks\n",
    "    \"\"\"\n",
    "    total_accuracy = 0.0\n",
    "    valid_tasks = 0\n",
    "    \n",
    "    for task in tasks:\n",
    "        try:\n",
    "            # Prepare support and query sets\n",
    "            support_graphs = task.train_graphs\n",
    "            query_graphs = task.test_graphs\n",
    "            \n",
    "            if not support_graphs or not query_graphs:\n",
    "                continue\n",
    "                \n",
    "            # Create batches\n",
    "            support_batch = Batch.from_data_list(support_graphs).to(device)\n",
    "            query_batch = Batch.from_data_list(query_graphs).to(device)\n",
    "            \n",
    "            # Evaluate with few-shot adaptation\n",
    "            accuracy = trainer.validate_task(support_batch, query_batch, task)\n",
    "            total_accuracy += accuracy\n",
    "            valid_tasks += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating task {task.task_id}: {e}\")\n",
    "    \n",
    "    return total_accuracy / valid_tasks if valid_tasks > 0 else 0.0\n",
    "\n",
    "# Plot k-fold results\n",
    "def plot_kfold_results(fold_results, model_type, method, save_path=None):\n",
    "    \"\"\"Plot k-fold validation results\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Extract data\n",
    "    fold_indices = [fold[\"fold\"] for fold in fold_results]\n",
    "    val_accuracies = [fold[\"val_accuracy\"] for fold in fold_results]\n",
    "    \n",
    "    # Plot validation accuracies\n",
    "    plt.bar(fold_indices, val_accuracies, alpha=0.7)\n",
    "    \n",
    "    # Add average line\n",
    "    avg_acc = np.mean(val_accuracies)\n",
    "    plt.axhline(y=avg_acc, color='r', linestyle='--', label=f'Average: {avg_acc:.4f}')\n",
    "    \n",
    "    # Labels and title\n",
    "    plt.xlabel('Fold')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.title(f'{model_type.upper()} with {method.upper()} K-Fold Cross-Validation Results')\n",
    "    plt.xticks(fold_indices)\n",
    "    plt.ylim([0, 1.0])\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Plot saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Compare results across different models and methods\n",
    "def compare_results(results, save_path=None):\n",
    "    \"\"\"\n",
    "    Compare results across different models and methods.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary mapping (model_type, method) to result dicts\n",
    "        save_path: Path to save the comparison plot\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Extract data\n",
    "    models = []\n",
    "    val_accs = []\n",
    "    test_accs = []\n",
    "    \n",
    "    for key, result in results.items():\n",
    "        model_type, method = key\n",
    "        models.append(f\"{model_type}-{method}\")\n",
    "        val_accs.append(result[\"avg_val_accuracy\"])\n",
    "        test_accs.append(result[\"test_accuracy\"])\n",
    "    \n",
    "    # Set positions for bars\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    # Create grouped bars\n",
    "    plt.bar(x - width/2, val_accs, width, label='Validation Accuracy', color='skyblue')\n",
    "    plt.bar(x + width/2, test_accs, width, label='Test Accuracy', color='lightcoral')\n",
    "    \n",
    "    # Add details\n",
    "    plt.xlabel('Model-Method')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Comparison of Models and Meta-Learning Methods')\n",
    "    plt.xticks(x, models, rotation=45, ha='right')\n",
    "    plt.ylim([0, 1.0])\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Comparison plot saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    try:\n",
    "        print(\"Starting k-fold cross-validation...\")\n",
    "        all_results = {}\n",
    "        \n",
    "        # Run k-fold training for NLM with MAML\n",
    "        nlm_maml_results = train_kfold_meta_learning(\n",
    "            model_type=\"nlm\",\n",
    "            model_path=\"output/models/nlm/nlm_reasoning_module_final.pt\",\n",
    "            train_dir=TRAIN_DIR,\n",
    "            test_dir=EVAL_DIR,\n",
    "            k=5,\n",
    "            method=\"maml\"\n",
    "        )\n",
    "        all_results[(\"nlm\", \"maml\")] = nlm_maml_results\n",
    "        \n",
    "        # Plot NLM-MAML k-fold results\n",
    "        plot_kfold_results(\n",
    "            nlm_maml_results[\"fold_results\"], \n",
    "            \"nlm\",\n",
    "            \"maml\",\n",
    "            save_path=os.path.join(METRICS_DIR, \"nlm\", \"maml_results.png\")\n",
    "        )\n",
    "        \n",
    "        # # Run k-fold training for NLM with Prototypical Networks\n",
    "        # nlm_proto_results = train_kfold_meta_learning(\n",
    "        #     model_type=\"nlm\",\n",
    "        #     train_dir=TRAIN_DIR,\n",
    "        #     test_dir=EVAL_DIR,\n",
    "        #     k=5,\n",
    "        #     method=\"proto\"\n",
    "        # )\n",
    "        # all_results[(\"nlm\", \"proto\")] = nlm_proto_results\n",
    "        \n",
    "        # # Plot NLM-Proto k-fold results\n",
    "        # plot_kfold_results(\n",
    "        #     nlm_proto_results[\"fold_results\"], \n",
    "        #     \"nlm\",\n",
    "        #     \"proto\",\n",
    "        #     save_path=os.path.join(METRICS_DIR, \"nlm_proto_kfold_results.png\")\n",
    "        # )\n",
    "        \n",
    "        # Run k-fold training for Unified with MAML\n",
    "        unified_maml_results = train_kfold_meta_learning(\n",
    "            model_type=\"unified\",\n",
    "            model_path=\"output/models/unified/unified_reasoning_model_final.pt\",\n",
    "            train_dir=TRAIN_DIR,\n",
    "            test_dir=EVAL_DIR,\n",
    "            k=5,\n",
    "            method=\"maml\"\n",
    "        )\n",
    "        all_results[(\"unified\", \"maml\")] = unified_maml_results\n",
    "        \n",
    "        # Plot Unified-MAML k-fold results\n",
    "        plot_kfold_results(\n",
    "            unified_maml_results[\"fold_results\"], \n",
    "            \"unified\",\n",
    "            \"maml\",\n",
    "            save_path=os.path.join(METRICS_DIR, \"unified\", \"maml_results.png\")\n",
    "        )\n",
    "        \n",
    "        # # Run k-fold training for Unified with Prototypical Networks\n",
    "        # unified_proto_results = train_kfold_meta_learning(\n",
    "        #     model_type=\"unified\",\n",
    "        #     train_dir=TRAIN_DIR,\n",
    "        #     test_dir=EVAL_DIR,\n",
    "        #     k=5,\n",
    "        #     method=\"proto\"\n",
    "        # )\n",
    "        # all_results[(\"unified\", \"proto\")] = unified_proto_results\n",
    "        \n",
    "        # # Plot Unified-Proto k-fold results\n",
    "        # plot_kfold_results(\n",
    "        #     unified_proto_results[\"fold_results\"], \n",
    "        #     \"unified\",\n",
    "        #     \"proto\",\n",
    "        #     save_path=os.path.join(METRICS_DIR, \"unified_proto_kfold_results.png\")\n",
    "        # )\n",
    "        \n",
    "        # Compare all results\n",
    "        compare_results(all_results, save_path=os.path.join(METRICS_DIR, \"all_models_comparison.png\"))\n",
    "        \n",
    "        # Print summary of results\n",
    "        print(\"\\n----- RESULTS SUMMARY -----\")\n",
    "        for (model_type, method), results in all_results.items():\n",
    "            print(f\"{model_type.upper()} with {method.upper()}: Val Acc = {results['avg_val_accuracy']:.4f}, Test Acc = {results['test_accuracy']:.4f}\")\n",
    "        \n",
    "        print(\"\\nK-fold meta-learning training and evaluation complete!\")\n",
    "        return all_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main function: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Run the main function when executed\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
